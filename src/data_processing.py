"""
Module de traitement des donnÃ©es pour le systÃ¨me de crÃ©dit scoring.
Suit le workflow ML dÃ©fini dans l'architecture.
"""

import pandas as pd
import numpy as np
import os
import logging
from typing import Tuple, Dict, Any

class DataProcessor:
    """
    Classe principale pour le chargement et prÃ©traitement des donnÃ©es.
    Suit l'ordre exact du workflow ML dÃ©fini dans l'architecture.
    """
    
    def __init__(self):
        """Initialisation du processeur de donnÃ©es"""
        self.data = None
        self.cleaned_data = None
        self.quality_report = {}
        
        # Configuration du logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def load_data(self, file_path: str = "data/raw/credit.csv") -> pd.DataFrame:
        """
        Ã‰TAPE 1.1: Chargement et validation initiale des donnÃ©es
        """
        print("ğŸ”„ Ã‰TAPE 1.1: CHARGEMENT DES DONNÃ‰ES")
        print("=" * 50)
        
        # Chargement des donnÃ©es
        try:
            self.data = pd.read_csv(file_path)
            print(f"âœ… DonnÃ©es chargÃ©es: {len(self.data)} lignes, {len(self.data.columns)} colonnes")
        except Exception as e:
            print(f"âŒ Erreur de chargement: {e}")
            return None
            
        # Validation de l'intÃ©gritÃ© du fichier
        self.validate_file_integrity()
        
        # VÃ©rification de conformitÃ© du schÃ©ma
        self.check_schema_compliance()
        
        # RÃ©sumÃ© statistique
        self.log_data_summary()
        
        # GÃ©nÃ©ration du rapport qualitÃ©
        self.generate_quality_report()
        
        return self.data
    
    def validate_file_integrity(self):
        """VÃ©rification de l'intÃ©gritÃ© du fichier"""
        print("\nğŸ” Validation de l'intÃ©gritÃ©...")
        
        if self.data is None or self.data.empty:
            print("âŒ Fichier vide ou non chargÃ©")
            return False
            
        print(f"âœ… IntÃ©gritÃ© validÃ©e: {len(self.data)} enregistrements")
        return True
    
    def check_schema_compliance(self):
        """VÃ©rification de conformitÃ© du schÃ©ma"""
        print("\nğŸ“‹ VÃ©rification du schÃ©ma...")
        
        # Colonnes attendues
        expected_columns = [
            'duree', 'historique', 'objet', 'montant', 'epargne',
            'anciennete_emploi', 'taux_endettement', 'statut', 
            'autres_debiteurs', 'domicile', 'biens', 'age',
            'credit_exterieur', 'logement', 'emploi', 'nombre_personnes',
            'telephone', 'cible', 'compte', 'nombre_credit', 'travailleur_etranger'
        ]
        
        missing_cols = set(expected_columns) - set(self.data.columns)
        
        if missing_cols:
            print(f"âš ï¸ Colonnes manquantes: {missing_cols}")
        else:
            print("âœ… SchÃ©ma conforme")
            
        return len(missing_cols) == 0
    
    def log_data_summary(self):
        """RÃ©sumÃ© statistique des donnÃ©es"""
        print("\nğŸ“Š RÃ©sumÃ© statistique...")
        
        summary = {
            'nb_lignes': len(self.data),
            'nb_colonnes': len(self.data.columns),
            'valeurs_manquantes': self.data.isnull().sum().sum(),
            'doublons': self.data.duplicated().sum()
        }
        
        for key, value in summary.items():
            print(f"   â€¢ {key}: {value}")
            
        self.quality_report['summary'] = summary
    
    def generate_quality_report(self):
        """GÃ©nÃ©ration du rapport qualitÃ© initial"""
        print("\nğŸ“ GÃ©nÃ©ration du rapport qualitÃ©...")
        
        # Analyse des valeurs manquantes
        missing_analysis = {}
        for col in self.data.columns:
            missing_count = self.data[col].isnull().sum()
            if missing_count > 0:
                missing_analysis[col] = {
                    'count': missing_count,
                    'percentage': (missing_count / len(self.data)) * 100
                }
        
        self.quality_report.update({
            'missing_values': missing_analysis,
            'duplicates': self.data.duplicated().sum()
        })
        
        print(f"âœ… Rapport qualitÃ© gÃ©nÃ©rÃ©")
        print(f"   â€¢ Valeurs manquantes: {len(missing_analysis)} colonnes affectÃ©es")
    
    def clean_data(self) -> pd.DataFrame:
        """
        Ã‰TAPE 1.2: Nettoyage des donnÃ©es
        """
        print("\nğŸ§¹ Ã‰TAPE 1.2: NETTOYAGE DES DONNÃ‰ES")
        print("=" * 50)
        
        if self.data is None:
            print("âŒ Aucune donnÃ©e Ã  nettoyer. Chargez d'abord les donnÃ©es.")
            return None
            
        # Copie pour le nettoyage
        self.cleaned_data = self.data.copy()
        
        # 1. Suppression des doublons
        self.remove_duplicates()
        
        # 2. Traitement des valeurs manquantes
        self.handle_missing_values()
        
        # 3. Traitement des valeurs aberrantes
        self.treat_outliers()
        
        # 4. Standardisation des formats
        self.standardize_formats()
        
        # 5. Nettoyage de la variable cible
        self.clean_target_variable()
        
        # 6. Validation finale
        self.validate_cleaned_data()
        
        return self.cleaned_data
    
    def remove_duplicates(self):
        """Suppression des doublons"""
        print("\nğŸ”„ Suppression des doublons...")
        
        initial_count = len(self.cleaned_data)
        self.cleaned_data = self.cleaned_data.drop_duplicates()
        removed_count = initial_count - len(self.cleaned_data)
        
        if removed_count > 0:
            print(f"âœ… {removed_count} doublons supprimÃ©s")
        else:
            print("âœ… Aucun doublon trouvÃ©")
    
    def handle_missing_values(self):
        """Traitement des valeurs manquantes"""
        print("\nğŸ•³ï¸ Traitement des valeurs manquantes...")
        
        missing_before = self.cleaned_data.isnull().sum().sum()
        
        # StratÃ©gies par type de variable
        for col in self.cleaned_data.columns:
            missing_count = self.cleaned_data[col].isnull().sum()
            
            if missing_count > 0:
                if self.cleaned_data[col].dtype in ['int64', 'float64']:
                    # Variables numÃ©riques: imputation par la mÃ©diane
                    median_value = self.cleaned_data[col].median()
                    self.cleaned_data[col].fillna(median_value, inplace=True)
                    print(f"   â€¢ {col}: {missing_count} valeurs â†’ mÃ©diane ({median_value:.1f})")
                    
                else:
                    # Variables catÃ©gorielles: imputation par le mode
                    mode_value = self.cleaned_data[col].mode().iloc[0] if not self.cleaned_data[col].mode().empty else 'unknown'
                    self.cleaned_data[col].fillna(mode_value, inplace=True)
                    print(f"   â€¢ {col}: {missing_count} valeurs â†’ mode ('{mode_value}')")
        
        missing_after = self.cleaned_data.isnull().sum().sum()
        print(f"âœ… Valeurs manquantes: {missing_before} â†’ {missing_after}")
    
    def treat_outliers(self):
        """Traitement des valeurs aberrantes"""
        print("\nğŸ“ˆ Traitement des valeurs aberrantes...")
        
        # Variables numÃ©riques Ã  traiter
        numeric_cols = ['duree', 'montant', 'age']
        
        for col in numeric_cols:
            if col in self.cleaned_data.columns:
                # MÃ©thode IQR pour dÃ©tecter les outliers
                Q1 = self.cleaned_data[col].quantile(0.25)
                Q3 = self.cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # Compter les outliers
                outliers_mask = (self.cleaned_data[col] < lower_bound) | (self.cleaned_data[col] > upper_bound)
                outliers_count = outliers_mask.sum()
                
                if outliers_count > 0:
                    # Ã‰crÃªtage des valeurs aberrantes (winsorization)
                    self.cleaned_data.loc[self.cleaned_data[col] < lower_bound, col] = lower_bound
                    self.cleaned_data.loc[self.cleaned_data[col] > upper_bound, col] = upper_bound
                    print(f"   â€¢ {col}: {outliers_count} outliers Ã©crÃªtÃ©s")
                else:
                    print(f"   â€¢ {col}: aucun outlier dÃ©tectÃ©")
    
    def standardize_formats(self):
        """Standardisation des formats"""
        print("\nğŸ”§ Standardisation des formats...")
        
        # Nettoyage des chaÃ®nes de caractÃ¨res
        for col in self.cleaned_data.columns:
            if self.cleaned_data[col].dtype == 'object':
                # Suppression des espaces en dÃ©but/fin
                self.cleaned_data[col] = self.cleaned_data[col].astype(str).str.strip()
                # Conversion en minuscules pour cohÃ©rence
                self.cleaned_data[col] = self.cleaned_data[col].str.lower()
        
        print("âœ… Formats standardisÃ©s")
    
    def clean_target_variable(self):
        """
        Nettoie et encode la variable cible 'classe'
        """
        self.logger.info("ğŸ¯ Nettoyage de la variable cible...")
        
        # Mapping des valeurs textuelles vers numÃ©riques pour la variable cible
        target_mapping = {
            'credit bien rembourse': 1,
            'credit mal rembourse': 0,
            'bon credit': 1,
            'mauvais credit': 0,
            'good': 1,
            'bad': 0,
            1: 1,
            0: 0
        }
        
        if 'classe' in self.cleaned_data.columns:
            # Convertir les valeurs selon le mapping
            self.cleaned_data['classe'] = self.cleaned_data['classe'].map(target_mapping)
            
            # VÃ©rifier s'il reste des valeurs non mappÃ©es
            unmapped = self.cleaned_data['classe'].isna().sum()
            if unmapped > 0:
                self.logger.warning(f"âš ï¸ {unmapped} valeurs non mappÃ©es dans 'classe'")
                # Les valeurs non mappÃ©es deviennent 0 par dÃ©faut
                self.cleaned_data['classe'] = self.cleaned_data['classe'].fillna(0)
            
            self.logger.info(f"âœ… Variable cible nettoyÃ©e: {self.cleaned_data['classe'].value_counts().to_dict()}")
        
        return self.cleaned_data
    
    def validate_cleaned_data(self):
        """Validation finale des donnÃ©es nettoyÃ©es"""
        print("\nâœ… Validation finale...")
        
        # VÃ©rifications finales
        checks = {
            'Valeurs manquantes': self.cleaned_data.isnull().sum().sum(),
            'Doublons': self.cleaned_data.duplicated().sum(),
            'Lignes finales': len(self.cleaned_data)
        }
        
        for check, value in checks.items():
            print(f"   â€¢ {check}: {value}")
        
        # Sauvegarde optionnelle
        output_path = "data/processed/credit_cleaned.csv"
        os.makedirs("data/processed", exist_ok=True)
        self.cleaned_data.to_csv(output_path, index=False)
        print(f"ğŸ’¾ DonnÃ©es nettoyÃ©es sauvegardÃ©es: {output_path}")
        
        return True
    
    def get_quality_report(self) -> Dict[str, Any]:
        """Retourne le rapport qualitÃ©"""
        return self.quality_report
    
    def get_cleaned_data(self) -> pd.DataFrame:
        """Retourne les donnÃ©es nettoyÃ©es"""
        return self.cleaned_data 