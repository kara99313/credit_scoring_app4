"""
Training Pipeline for Credit Scoring System

This module implements the complete training pipeline according to 
the specifications defined in PROJET_COMPLET_SPECIFICATION.md - √âTAPE 5

Author: Credit Scoring Team
Created: 2024
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
import joblib
from pathlib import Path
from typing import Dict, Tuple, Optional, Any
from datetime import datetime

# Add src to Python path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.calibration import CalibratedClassifierCV
import matplotlib.pyplot as plt
import seaborn as sns

try:
    import mlflow
    import mlflow.sklearn
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logging.warning("MLflow not available. Experiment tracking disabled.")


class TrainingPipeline:
    """
    √âTAPE 5: Pipeline d'entra√Ænement du mod√®le
    
    Impl√©mente l'entra√Ænement selon l'architecture d√©finie :
    - Optimisation des hyperparam√®tres
    - √âvaluation des performances
    - Calibration du mod√®le
    - Sauvegarde et versioning
    """
    
    def __init__(self, config: Dict):
        """
        Initialisation du pipeline d'entra√Ænement
        
        Args:
            config: Configuration du projet
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Configuration par d√©faut
        self.model_config = config.get('model', {})
        self.default_params = {
            'C': 1.0,
            'penalty': 'l2',
            'solver': 'lbfgs',
            'max_iter': 1000,
            'random_state': 42,
            'class_weight': 'balanced'
        }
        
        # Paths
        self.data_path = Path("data/processed")
        self.models_path = Path("models")
        self.models_path.mkdir(exist_ok=True)
        
        # M√©triques
        self.metrics = {}
        self.best_model = None
        self.best_params = None
        
    def run(self, experiment_name: Optional[str] = None, 
            hyperparameter_tuning: bool = True) -> Dict[str, Any]:
        """
        Ex√©cute le pipeline d'entra√Ænement complet
        
        Args:
            experiment_name: Nom de l'exp√©rience MLflow
            hyperparameter_tuning: Activer l'optimisation des hyperparam√®tres
            
        Returns:
            Dictionnaire contenant les r√©sultats d'entra√Ænement
        """
        print("\nüöÄ √âTAPE 5: PIPELINE D'ENTRA√éNEMENT DU MOD√àLE")
        print("=" * 60)
        
        # 1. Chargement des donn√©es
        print("\nüìä 1. Chargement des donn√©es...")
        X_train, X_test, y_train, y_test = self._load_and_split_data()
        
        # 2. Configuration MLflow
        if MLFLOW_AVAILABLE and experiment_name:
            self._setup_mlflow(experiment_name)
        
        # 3. Entra√Ænement du mod√®le
        print("\nüîß 2. Entra√Ænement du mod√®le...")
        if hyperparameter_tuning:
            model = self._train_with_hyperparameter_tuning(X_train, y_train)
        else:
            model = self._train_basic_model(X_train, y_train)
        
        # 4. √âvaluation
        print("\nüìà 3. √âvaluation du mod√®le...")
        metrics = self._evaluate_model(model, X_test, y_test)
        
        # 5. Calibration
        print("\n‚öñÔ∏è 4. Calibration du mod√®le...")
        calibrated_model = self._calibrate_model(model, X_train, y_train)
        
        # 6. Sauvegarde
        print("\nüíæ 5. Sauvegarde du mod√®le...")
        model_path = self._save_model(calibrated_model, metrics)
        
        # 7. G√©n√©ration du rapport
        print("\nüìã 6. G√©n√©ration du rapport...")
        report_path = self._generate_report(metrics, model_path)
        
        results = {
            'model': calibrated_model,
            'model_path': model_path,
            'metrics': metrics,
            'report_path': report_path,
            'best_params': self.best_params
        }
        
        print(f"\n‚úÖ Pipeline d'entra√Ænement termin√© avec succ√®s!")
        print(f"üìä AUC-ROC: {metrics.get('auc_roc', 'N/A'):.4f}")
        print(f"üéØ Accuracy: {metrics.get('accuracy', 'N/A'):.4f}")
        print(f"üíæ Mod√®le sauvegard√©: {model_path}")
        
        return results
    
    def _load_and_split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """Charge et divise les donn√©es"""
        
        # Chargement des donn√©es transform√©es
        data_file = self.data_path / "credit_engineered_transformed.csv"
        if not data_file.exists():
            raise FileNotFoundError(f"Fichier de donn√©es non trouv√©: {data_file}")
        
        df = pd.read_csv(data_file)
        
        # S√©paration des features et de la cible
        X = df.drop(columns=['cible'])
        y = df['cible']
        
        # Division train/test
        test_size = self.config.get('model', {}).get('test_size', 0.2)
        random_state = self.config.get('model', {}).get('random_state', 42)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, 
            stratify=y
        )
        
        print(f"   ‚úÖ Donn√©es charg√©es: {len(df)} √©chantillons")
        print(f"   ‚úÖ Train: {len(X_train)} √©chantillons")
        print(f"   ‚úÖ Test: {len(X_test)} √©chantillons")
        print(f"   ‚úÖ Features: {len(X.columns)} variables")
        
        return X_train, X_test, y_train, y_test
    
    def _train_with_hyperparameter_tuning(self, X_train: pd.DataFrame, 
                                         y_train: pd.Series) -> Any:
        """Entra√Æne le mod√®le avec optimisation des hyperparam√®tres"""
        
        print("   üîç Optimisation des hyperparam√®tres...")
        
        # Grille de param√®tres
        param_grid = {
            'C': [0.01, 0.1, 1.0, 10.0, 100.0],
            'penalty': ['l1', 'l2', 'elasticnet'],
            'solver': ['liblinear', 'lbfgs', 'saga'],
            'max_iter': [1000, 2000, 3000],
            'class_weight': [None, 'balanced']
        }
        
        # Mod√®le de base
        base_model = LogisticRegression(random_state=42)
        
        # GridSearchCV
        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        # Entra√Ænement
        grid_search.fit(X_train, y_train)
        
        self.best_params = grid_search.best_params_
        self.best_model = grid_search.best_estimator_
        
        print(f"   ‚úÖ Meilleurs param√®tres: {self.best_params}")
        print(f"   ‚úÖ Meilleur score CV: {grid_search.best_score_:.4f}")
        
        return self.best_model
    
    def _train_basic_model(self, X_train: pd.DataFrame, y_train: pd.Series) -> Any:
        """Entra√Æne un mod√®le de base sans optimisation"""
        
        print("   üîß Entra√Ænement du mod√®le de base...")
        
        # Param√®tres par d√©faut
        params = {**self.default_params, **self.model_config.get('params', {})}
        
        # Entra√Ænement
        model = LogisticRegression(**params)
        model.fit(X_train, y_train)
        
        self.best_model = model
        self.best_params = params
        
        print(f"   ‚úÖ Mod√®le entra√Æn√© avec param√®tres: {params}")
        
        return model
    
    def _evaluate_model(self, model: Any, X_test: pd.DataFrame, 
                       y_test: pd.Series) -> Dict[str, float]:
        """√âvalue les performances du mod√®le"""
        
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        # M√©triques de classification
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'auc_roc': roc_auc_score(y_test, y_proba)
        }
        
        # M√©triques m√©tier
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        
        metrics.update({
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'precision_0': tn / (tn + fn) if (tn + fn) > 0 else 0,
            'precision_1': tp / (tp + fp) if (tp + fp) > 0 else 0
        })
        
        # Calcul KS Statistic
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        ks_stat = np.max(tpr - fpr)
        metrics['ks_statistic'] = ks_stat
        
        # Calcul Gini
        metrics['gini_coefficient'] = 2 * metrics['auc_roc'] - 1
        
        self.metrics = metrics
        
        print(f"   ‚úÖ AUC-ROC: {metrics['auc_roc']:.4f}")
        print(f"   ‚úÖ Accuracy: {metrics['accuracy']:.4f}")
        print(f"   ‚úÖ Precision: {metrics['precision']:.4f}")
        print(f"   ‚úÖ Recall: {metrics['recall']:.4f}")
        print(f"   ‚úÖ F1-Score: {metrics['f1_score']:.4f}")
        print(f"   ‚úÖ KS Statistic: {metrics['ks_statistic']:.4f}")
        print(f"   ‚úÖ Gini Coefficient: {metrics['gini_coefficient']:.4f}")
        
        return metrics
    
    def _calibrate_model(self, model: Any, X_train: pd.DataFrame, 
                        y_train: pd.Series) -> Any:
        """Calibre le mod√®le pour am√©liorer les probabilit√©s"""
        
        print("   ‚öñÔ∏è Calibration du mod√®le...")
        
        # Calibration avec validation crois√©e
        calibrated_model = CalibratedClassifierCV(
            model, 
            method='isotonic',  # ou 'sigmoid'
            cv=3
        )
        
        calibrated_model.fit(X_train, y_train)
        
        print("   ‚úÖ Mod√®le calibr√© avec succ√®s")
        
        return calibrated_model
    
    def _save_model(self, model: Any, metrics: Dict[str, float]) -> str:
        """Sauvegarde le mod√®le entra√Æn√©"""
        
        # Cr√©ation du nom de fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        auc_score = metrics.get('auc_roc', 0)
        
        model_filename = f"credit_scoring_model_{timestamp}_auc{auc_score:.4f}.pkl"
        model_path = self.models_path / model_filename
        
        # Sauvegarde avec joblib
        model_info = {
            'model': model,
            'metrics': metrics,
            'params': self.best_params,
            'timestamp': timestamp,
            'version': '1.0'
        }
        
        joblib.dump(model_info, model_path)
        
        # Sauvegarde du meilleur mod√®le
        best_model_path = self.models_path / "best_model.pkl"
        joblib.dump(model_info, best_model_path)
        
        print(f"   ‚úÖ Mod√®le sauvegard√©: {model_path}")
        print(f"   ‚úÖ Meilleur mod√®le: {best_model_path}")
        
        return str(model_path)
    
    def _generate_report(self, metrics: Dict[str, float], model_path: str) -> str:
        """G√©n√®re un rapport d'entra√Ænement"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"training_report_{timestamp}.txt"
        report_path = Path("reports") / report_filename
        
        # Cr√©ation du r√©pertoire reports
        report_path.parent.mkdir(exist_ok=True)
        
        # G√©n√©ration du rapport
        report_content = f"""
RAPPORT D'ENTRA√éNEMENT - MOD√àLE DE CREDIT SCORING
=================================================

Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Mod√®le: {model_path}

PARAM√àTRES OPTIMAUX:
{'-' * 20}
{self._format_params(self.best_params)}

M√âTRIQUES DE PERFORMANCE:
{'-' * 25}
AUC-ROC: {metrics.get('auc_roc', 'N/A'):.4f}
Accuracy: {metrics.get('accuracy', 'N/A'):.4f}
Precision: {metrics.get('precision', 'N/A'):.4f}
Recall: {metrics.get('recall', 'N/A'):.4f}
F1-Score: {metrics.get('f1_score', 'N/A'):.4f}
Specificity: {metrics.get('specificity', 'N/A'):.4f}
Sensitivity: {metrics.get('sensitivity', 'N/A'):.4f}

M√âTRIQUES M√âTIER:
{'-' * 17}
KS Statistic: {metrics.get('ks_statistic', 'N/A'):.4f}
Gini Coefficient: {metrics.get('gini_coefficient', 'N/A'):.4f}

INTERPR√âTATION:
{'-' * 15}
- AUC-ROC > 0.7: Mod√®le acceptable
- AUC-ROC > 0.8: Bon mod√®le
- AUC-ROC > 0.9: Excellent mod√®le
- KS > 0.3: Pouvoir discriminant acceptable
- Gini > 0.4: Pouvoir pr√©dictif acceptable

STATUS: {'‚úÖ MOD√àLE ACCEPTABLE' if metrics.get('auc_roc', 0) > 0.7 else '‚ö†Ô∏è MOD√àLE √Ä AM√âLIORER'}
"""
        
        # Sauvegarde du rapport
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"   ‚úÖ Rapport g√©n√©r√©: {report_path}")
        
        return str(report_path)
    
    def _format_params(self, params: Dict) -> str:
        """Formate les param√®tres pour le rapport"""
        return '\n'.join([f"{k}: {v}" for k, v in params.items()])
    
    def _setup_mlflow(self, experiment_name: str):
        """Configure MLflow pour le tracking"""
        if not MLFLOW_AVAILABLE:
            return
        
        mlflow.set_experiment(experiment_name)
        mlflow.start_run()
        
        # Log des param√®tres
        if self.best_params:
            mlflow.log_params(self.best_params)
        
        # Log des m√©triques
        if self.metrics:
            mlflow.log_metrics(self.metrics)
        
        print(f"   ‚úÖ MLflow configur√©: {experiment_name}") 